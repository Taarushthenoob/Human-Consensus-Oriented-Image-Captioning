{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Link to the Github Repo:\nhttps://github.com/shikha-16/Human-Consensus-Oriented-Image-Captioning","metadata":{}},{"cell_type":"code","source":"!pip install pytorch-pretrained-bert\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms, datasets, models\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport re\nfrom scipy import ndimage\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport numpy as np\n\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel\nfrom tqdm import tqdm, trange\nimport pandas as pd\nimport io\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-19T17:38:41.752480Z","iopub.execute_input":"2022-05-19T17:38:41.752793Z","iopub.status.idle":"2022-05-19T17:39:01.293719Z","shell.execute_reply.started":"2022-05-19T17:38:41.752713Z","shell.execute_reply":"2022-05-19T17:39:01.292886Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:39:01.295721Z","iopub.execute_input":"2022-05-19T17:39:01.295974Z","iopub.status.idle":"2022-05-19T17:39:01.360635Z","shell.execute_reply.started":"2022-05-19T17:39:01.295938Z","shell.execute_reply":"2022-05-19T17:39:01.359859Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"scaler = transforms.Resize([224, 224])\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\nto_tensor = transforms.ToTensor()\n\nfeature_extraction = torchvision.models.resnet18(pretrained=True).to(device)\nfeature_extraction = nn.Sequential(*list(feature_extraction.children())[:-2]).to(device)\n\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_WE = BertModel.from_pretrained('bert-base-uncased').to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:39:01.361777Z","iopub.execute_input":"2022-05-19T17:39:01.362199Z","iopub.status.idle":"2022-05-19T17:39:29.481887Z","shell.execute_reply.started":"2022-05-19T17:39:01.362157Z","shell.execute_reply":"2022-05-19T17:39:29.480997Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"for param in feature_extraction.parameters():\n    param.requires_grad_(False)\nfeature_extraction.eval()\n\ndef img_load_feat(img_loc,img_name):\n    img_loc += str(img_name) + '.jpg'\n    img = Image.open(img_loc)\n    t_img = normalize(to_tensor(scaler(img)))\n    t_img = t_img.to(device)\n    t_img = torch.unsqueeze(t_img, 0)\n    feature = feature_extraction(t_img)\n    return feature","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:39:29.484548Z","iopub.execute_input":"2022-05-19T17:39:29.485091Z","iopub.status.idle":"2022-05-19T17:39:29.493835Z","shell.execute_reply.started":"2022-05-19T17:39:29.485050Z","shell.execute_reply":"2022-05-19T17:39:29.493173Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"img_loc = '../input/flickr8k/Images/'\nimg_features = dict()\nfor img_id_jpg in os.listdir(img_loc):\n    img_id = img_id_jpg.split('.')[0]\n    img_features[img_id] = img_load_feat(img_loc, img_id)\n    img_features[img_id].requires_grad_()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:39:29.495270Z","iopub.execute_input":"2022-05-19T17:39:29.495778Z","iopub.status.idle":"2022-05-19T17:41:56.384745Z","shell.execute_reply.started":"2022-05-19T17:39:29.495744Z","shell.execute_reply":"2022-05-19T17:41:56.383931Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_cider(captions):\n\n    cider = dict()\n    tfidfVectorizers=[]\n    for i in range(4):\n        tf_idf_vect = TfidfVectorizer(ngram_range = (i+1,i+1))\n        tfidfVectorizers.append(tf_idf_vect)\n    for key in captions:\n        corpus = captions[key]\n        cider[key] = []\n        \n        tfidf_matrix = tfidfVectorizers[0].fit_transform(corpus)        \n        tfidf_mat = tfidfVectorizers[0].fit_transform(corpus)\n        mean_cider=0\n        for i in range(len(corpus)):\n            mean_cider=0\n            for n in range(4):\n                tfidf_mat = tfidfVectorizers[0].fit_transform(corpus)\n                tfidf_mat = tfidf_mat.todense().tolist()\n                r = tfidf_mat[i] \n                r_norm = np.linalg.norm(np.array(r))\n                sum = 0\n                for j in range(len(corpus)):\n                    if i != j:\n                        vec_norm = np.linalg.norm(np.array(tfidf_mat[j]))\n                        sum += (np.dot(r,tfidf_mat[j])/(np.sqrt(r_norm*vec_norm)))\n                mean_cider += (sum/(len(corpus)-1))\n            mean_cider /= 4.0\n            cider[key].append(mean_cider)\n            \n    return cider","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:41:56.386349Z","iopub.execute_input":"2022-05-19T17:41:56.386588Z","iopub.status.idle":"2022-05-19T17:41:56.398343Z","shell.execute_reply.started":"2022-05-19T17:41:56.386554Z","shell.execute_reply":"2022-05-19T17:41:56.397656Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"file = open('../input/flickr8k/captions.txt', 'r')\nip_desc = file.read()\nfile.close()\n\nimg_cap = dict()\ndata = list()\nmax_len = 35\nindex = 1\nmy_vocab = dict()\nmy_rev_vocab = dict()\n\nmy_vocab[\"[PAD]\"] = 0\nmy_rev_vocab[0] = \"[PAD]\"\n\nfor line in ip_desc.split('\\n')[1:-1]:\n    if '\"' in line:\n        ip = re.split(r',(?=\")',line)\n    else:\n        ip = line.split(',')\n    \n    # image name\n    img_id = ip[0].split('.')[0]\n    \n    # cleaning desc\n    clean_desc = ''\n    for ch in ip[1]:\n        if ('A'<=ch and ch<='Z') or ('a'<=ch and ch<='z') or ch==' ':\n            clean_desc += ch\n    clean_desc = \"[CLS] \" + clean_desc.rstrip().lower() + \" [SEP]\"\n    \n    # truncating sentences with len > max_len\n    if len(clean_desc) > max_len:\n        clean_desc = clean_desc[:max_len]\n    \n    # mapping each img_id to a list of 5 captions\n    if img_id not in img_cap:\n        img_cap[img_id] = list()\n    img_cap[img_id].append(clean_desc)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:41:56.399867Z","iopub.execute_input":"2022-05-19T17:41:56.400144Z","iopub.status.idle":"2022-05-19T17:41:57.551404Z","shell.execute_reply.started":"2022-05-19T17:41:56.400111Z","shell.execute_reply":"2022-05-19T17:41:57.550651Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"all_ciders = get_cider(img_cap)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:41:57.552797Z","iopub.execute_input":"2022-05-19T17:41:57.553058Z","iopub.status.idle":"2022-05-19T17:45:40.452032Z","shell.execute_reply.started":"2022-05-19T17:41:57.553024Z","shell.execute_reply":"2022-05-19T17:45:40.451259Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"for img_id in img_cap:\n    for cap_num in range(len(img_cap[img_id])):\n        clean_desc = img_cap[img_id][cap_num]\n        # tokenization of clean desc\n        tok_desc = clean_desc.split()\n        # tok_desc = tokenizer.tokenize(clean_desc)    \n\n        # building vocab\n        tok_desc_ind = list()\n        for tok in tok_desc:\n            if tok not in my_vocab:\n                my_vocab[tok] = index\n                my_rev_vocab[index] = tok\n                index += 1\n            tok_desc_ind.append(my_vocab[tok])\n\n        # data\n        for i in range(1,len(tok_desc_ind)-1):\n            data.append([[img_id,tok_desc_ind[:i]],tok_desc_ind[i],all_ciders[img_id][cap_num]])\n\n        tok_desc_ind += [0]*(max_len-len(tok_desc))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:45:40.453383Z","iopub.execute_input":"2022-05-19T17:45:40.453619Z","iopub.status.idle":"2022-05-19T17:45:41.493855Z","shell.execute_reply.started":"2022-05-19T17:45:40.453587Z","shell.execute_reply":"2022-05-19T17:45:41.493116Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_data = data[:(80*len(data))//100]\ntest_data = data[(80*len(data))//100:]\n# train_data, test_data = torch.utils.data.random_split(img_ids, [int(0.8*len(img_ids)),len(img_ids)-int(0.8*len(img_ids))])\n\nprint(index, len(img_features.keys()), len(train_data))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:57:00.749816Z","iopub.execute_input":"2022-05-19T17:57:00.750088Z","iopub.status.idle":"2022-05-19T17:57:00.760894Z","shell.execute_reply.started":"2022-05-19T17:57:00.750059Z","shell.execute_reply":"2022-05-19T17:57:00.760174Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"class ConsensusLoss(torch.nn.Module):\n    \n    def _init_(self):\n        super(ConsensusLoss,self)._init_()\n        \n    def forward(self, outputs, targets, cider):\n        \n        den=0\n        op = torch.exp(outputs[0])\n        den = torch.sum(op)\n        op = torch.div(op, den)\n        \n        ce = -1*torch.log(op[targets[0]])\n        cl = (ce*cider)\n        \n        return cl","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:45:41.505202Z","iopub.execute_input":"2022-05-19T17:45:41.506065Z","iopub.status.idle":"2022-05-19T17:45:41.518253Z","shell.execute_reply.started":"2022-05-19T17:45:41.506028Z","shell.execute_reply":"2022-05-19T17:45:41.517424Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n    \n    def __init__(self):\n        super(SelfAttention,self).__init__()\n        \n        in_dim = 512\n        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.softmax  = nn.Softmax(dim=-1)\n        \n    def forward(self,x):\n        \"\"\"\n            inputs :\n                x : input feature maps( B X C X W X H)\n            returns :\n                out : self attention value + input feature \n                attention: B X N X N (N is Width*Height)\n        \"\"\"\n        \n        m_batchsize,C,width,height = x.size()\n        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n        energy =  torch.bmm(proj_query,proj_key) # transpose check\n        attention = self.softmax(energy) # BX (N) X (N) \n        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n\n        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n        out = out.view(m_batchsize,C,width,height)\n        \n        out = self.gamma*out + x\n        return out, attention","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:45:41.519624Z","iopub.execute_input":"2022-05-19T17:45:41.520282Z","iopub.status.idle":"2022-05-19T17:45:41.533411Z","shell.execute_reply.started":"2022-05-19T17:45:41.520239Z","shell.execute_reply":"2022-05-19T17:45:41.532699Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class BERT_WE(nn.Module):\n    \n    def __init__(self):\n        super(BERT_WE,self).__init__()\n        \n    def forward(desc):\n        att_mask = torch.IntTensor([[1]*len(desc) + [0]*(max_len-len(desc))]).to(device)\n        pad_desc = (desc + [0]*(max_len-len(desc)))\n        t_pad_desc = torch.IntTensor([pad_desc]).to(device)\n        bert_WE.eval()\n        embedding = (bert_WE(t_pad_desc, attention_mask = att_mask, output_all_encoded_layers = False))[0][0][-1]\n        return embedding","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:45:41.534926Z","iopub.execute_input":"2022-05-19T17:45:41.535518Z","iopub.status.idle":"2022-05-19T17:45:41.546564Z","shell.execute_reply.started":"2022-05-19T17:45:41.535479Z","shell.execute_reply":"2022-05-19T17:45:41.545882Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Caption_Generation(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        \n        self.attention = SelfAttention()\n        self.flatten = nn.Flatten()\n        self.img_proc = nn.Sequential(nn.Linear(7*7*512,4096), nn.Dropout(0.2), nn.ReLU(),\n                                    nn.Linear(4096,1024), nn.Dropout(0.2), nn.ReLU()\n                                   )\n        \n        self.embed = BERT_WE()\n        self.lstm = nn.LSTM(1792,1024)\n        \n        self.cap_proc = nn.Sequential(\n                                    nn.Linear(1024,512), nn.Dropout(0.2), nn.ReLU(),\n                                    nn.Linear(512,256), nn.Dropout(0.2), nn.ReLU(),\n                                    nn.Linear(256,index), nn.Dropout(0.2), nn.ReLU()\n                                   )\n                \n        self.cap_gen = nn.Sequential(\n            nn.Linear(256,128), nn.ReLU(),\n            nn.Linear(128,index), nn.ReLU()\n        )\n    \n    def forward(self, feat, desc):\n        \n        context, att_wts = self.attention(feat)\n        ip1 = self.flatten(context)\n        ip1 = self.img_proc(torch.tensor(ip1))\n        \n        att_mask = torch.IntTensor([[1]*len(desc) + [0]*(max_len-len(desc))]).to(device)\n        pad_desc = (desc + [0]*(max_len-len(desc)))\n        t_pad_desc = torch.IntTensor([pad_desc]).to(device)\n        bert_WE.eval()\n        ip2 = (bert_WE(t_pad_desc, attention_mask = att_mask, output_all_encoded_layers = False))[0][0][-1]\n        ip2 = torch.tensor(torch.unsqueeze(ip2,0)).to(device)\n        \n        ip = torch.cat([ip1, ip2], 1)\n        ip = self.lstm(ip)[0]\n        # print(type(ip))\n        out = self.cap_proc(ip)\n        \n        # ip = torch.unsqueeze(torch.cat((ip1[0],ip2[0]),0),0)\n        # out = self.cap_gen(ip)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:45:41.549315Z","iopub.execute_input":"2022-05-19T17:45:41.549521Z","iopub.status.idle":"2022-05-19T17:45:41.563716Z","shell.execute_reply.started":"2022-05-19T17:45:41.549498Z","shell.execute_reply":"2022-05-19T17:45:41.563059Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def train(train_data, epochs, model, optim, loss_f):\n    \n    model = model.to(device)\n    model.train()\n    loss_vals = list()\n    \n    for epoch in range(epochs):\n        train_loss = 0.0\n        idx = 0\n        for ip,op,score in train_data:\n            if ip[0] not in img_features:\n                continue\n            optim.zero_grad()\n            \n            output = model(img_features[ip[0]],ip[1]).to(device)\n            t_op = torch.LongTensor([op]).to(device)\n            loss = loss_f(output,t_op,score)\n            \n            if idx%100 == 0:\n                print(\"idx: \", idx)\n                print(\"img_id: \", ip[0])\n                img = Image.open(img_loc+ip[0]+'.jpg')\n                imgplot = plt.imshow(img)\n                plt.show()\n                \n                print(\"ip:\", end = ' ')\n                for word in ip[1]:\n                    print(my_rev_vocab[word], end = ' ')\n                print()\n                word = int(torch.max(output[-1].view(1,-1), 1)[1])\n                word = my_rev_vocab[word]\n                print(\"actual op: \", word)\n                print(\"target op:\", my_rev_vocab[op])\n                print(\"loss: \", loss.item())\n                print(\"\\n---------------------------------------\\n\")\n            idx += 1\n                \n            train_loss += loss\n            loss.backward()\n            optim.step()\n        \n        train_loss /= len(train_data)\n        print('Epoch: ', epoch, 'Avg Train loss: ', float(train_loss))\n        loss_vals.append(train_loss)\n        \n    plt.plot(np.linspace(1, epochs, epochs).astype(int), loss_vals)\n        \n\ndef test(img_loc, test_data, model, loss_f):\n    model.eval()\n    test_loss = 0.0\n    \n    for ip,op,score in train_data:\n        if ip[0] not in img_features:\n            continue\n\n        output = model(img_features[ip[0]],ip[1])\n        t_op = torch.LongTensor([op]).to(device)\n        loss = loss_f(output,t_op,score)\n        test_loss += loss\n        \n        # printing img, given input prefix sentence, predicted next word, expected next word\n        print(ip[0], loss.item())\n        img = Image.open(img_loc+ip[0]+'.jpg')\n        imgplot = plt.imshow(img)\n        plt.show()\n        print(\"ip:\", end = ' ')\n        for word in ip[1]:\n            print(my_rev_vocab[word], end = ' ')\n        print()\n        word = int(torch.max(output[-1].view(1,-1), 1)[1])\n        word = my_rev_vocab[word]\n        print(\"actual op: \", word)\n        print(\"target op:\", my_rev_vocab[op])\n        \n    test_loss /= len(test_data)\n    print('Epoch: ', epoch, 'Avg Test loss: ', float(test_loss))\n    \n    plt.plot(np.linspace(1, epochs, epochs).astype(int), loss_vals)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:09:50.683448Z","iopub.execute_input":"2022-05-19T18:09:50.683707Z","iopub.status.idle":"2022-05-19T18:09:50.702930Z","shell.execute_reply.started":"2022-05-19T18:09:50.683677Z","shell.execute_reply":"2022-05-19T18:09:50.702061Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"caption_generation = Caption_Generation().to(device)\n\nprint(\"Total Parameters: \", sum(p.numel() for p in caption_generation.parameters()))\nprint(\"Trainable Parameters: \", sum(p.numel() for p in caption_generation.parameters() if p.requires_grad))\nprint()\n\noptimizer = torch.optim.Adam(caption_generation.parameters(), lr = 5e-4)\nloss = ConsensusLoss().to(device)\ntrain(train_data[:2000], 1, caption_generation, optimizer, loss)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:57:12.333795Z","iopub.execute_input":"2022-05-19T17:57:12.334092Z","iopub.status.idle":"2022-05-19T17:58:37.823047Z","shell.execute_reply.started":"2022-05-19T17:57:12.334060Z","shell.execute_reply":"2022-05-19T17:58:37.819198Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"test(img_loc, test_data[:10], caption_generation, loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sample Test","metadata":{}},{"cell_type":"code","source":"def generate_caption(img_feature, caption, model):\n    model = model.to(device)\n    model.eval()\n    output = model(img_feature, caption).to(device)\n    next_word = torch.max(output[-1].view(1,-1), 1)[1].to(device)\n    if next_word == my_vocab[\"[SEP]\"] or len(caption) == max_len-1:\n        caption.append(my_vocab[\"[SEP]\"])\n        return caption\n    caption.append(next_word)\n    return generate_caption(img_feature, caption, model)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:10:42.119838Z","iopub.execute_input":"2022-05-19T18:10:42.120386Z","iopub.status.idle":"2022-05-19T18:10:42.126324Z","shell.execute_reply.started":"2022-05-19T18:10:42.120349Z","shell.execute_reply":"2022-05-19T18:10:42.125632Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# random.seed(5)\nimg_id = data[random.randint(0,len(data)-1)][0][0]\nprint(img_id)\nimg = Image.open(img_loc+img_id+'.jpg')\nimgplot = plt.imshow(img)\nplt.show()\n\ncaption = generate_caption(img_features[img_id], [my_vocab[\"[CLS]\"]], caption_generation)\nfor word_seq in caption:\n    print(my_rev_vocab[int(word_seq)], end = ' ')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:11:12.387378Z","iopub.execute_input":"2022-05-19T18:11:12.388014Z","iopub.status.idle":"2022-05-19T18:11:13.208691Z","shell.execute_reply.started":"2022-05-19T18:11:12.387958Z","shell.execute_reply":"2022-05-19T18:11:13.207922Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"References:\n* https://www.ijcai.org/proceedings/2020/0092.pdf\n* https://towardsdatascience.com/a-guide-to-image-captioning-e9fd5517f350\n* https://arxiv.org/pdf/1411.4555.pdf\n* https://www.analyticsvidhya.com/blog/2018/04/solving-an-image-captioning-task-using-deep-learning/\n* https://towardsdatascience.com/3-types-of-contextualized-word-embeddings-from-bert-using-transfer-learning-81fcefe3fe6d\n* https://arxiv.org/pdf/1502.03044.pdf\n\n\n* https://medium.com/analytics-vidhya/image-captioning-with-attention-part-1-e8a5f783f6d3\n* https://medium.com/analytics-vidhya/image-captioning-with-attention-part-2-f3616d5cf8d1\n\n\n* https://discuss.pytorch.org/t/attention-in-image-classification/80147\n* https://github.com/heykeetae/Self-Attention-GAN/blob/master/sagan_models.py#L8\n* https://arxiv.org/pdf/1805.08318.pdf","metadata":{}}]}