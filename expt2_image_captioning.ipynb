{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch-pretrained-bert\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms, datasets, models\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport re\nfrom scipy import ndimage\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport numpy as np\n\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel\nfrom tqdm import tqdm, trange\nimport pandas as pd\nimport io\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n# % matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = transforms.Resize([224, 224])\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\nto_tensor = transforms.ToTensor()\n\nfeature_extraction = torchvision.models.resnet18(pretrained=True).to(device)\nfeature_extraction = nn.Sequential(*list(feature_extraction.children())[:-2]).to(device)\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_WE = BertModel.from_pretrained('bert-base-uncased').to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param in feature_extraction.parameters():\n    param.requires_grad = False\nfeature_extraction.eval()\n\ndef img_load_feat(img_loc,img_name):\n    img_loc += str(img_name) + '.jpg'\n    img = Image.open(img_loc)\n    t_img = normalize(to_tensor(scaler(img)))\n    t_img = t_img.to(device)\n    t_img = torch.unsqueeze(t_img, 0)\n    # new\n    feature = feature_extraction(t_img)\n    return feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_loc = '../input/flickr8k/Images/'\nimg_features = dict()\nfor img_id_jpg in os.listdir(img_loc):\n    img_id = img_id_jpg.split('.')[0]\n    img_features[img_id] = img_load_feat(img_loc, img_id)\n    img_features[img_id].requires_grad_()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file = open('../input/flickr8k/captions.txt', 'r')\nip_desc = file.read()\nfile.close()\n\ndata = list()\nall_desc = dict()\nmax_len = 15\nindex = 0\nmy_vocab = dict()\nmy_rev_vocab = dict()\n\nfor line in ip_desc.split('\\n')[1:-1]:\n    if '\"' in line:\n        ip = re.split(r',(?=\")',line)\n    else:\n        ip = line.split(',')\n\n    img_id = ip[0].split('.')[0]\n        \n    # cleaning desc\n    clean_desc = ''\n    for ch in ip[1]:\n        if ('A'<=ch and ch<='Z') or ('a'<=ch and ch<='z') or ch==' ':\n            clean_desc += ch\n    clean_desc = clean_desc.rstrip().lower()\n    \n    if img_id not in all_desc:\n        all_desc[img_id] = list()\n    all_desc[img_id].append(clean_desc)\n    \n    if len(clean_desc) > 15:\n        clean_desc = clean_desc[:15]\n    \n    # tokenization of clean desc\n    tok_desc = tokenizer.tokenize(clean_desc)    \n    \n    for tok in tok_desc:\n        if tok not in my_vocab:\n            my_vocab[tok] = index\n            my_rev_vocab[index] = tok\n            index += 1\n    \n    for i in range(0,len(tok_desc)):\n        data.append([img_id,my_vocab[tok_desc[i]]])\n    \n    # converting tokens to IDs\n    tok_desc = tokenizer.convert_tokens_to_ids(tok_desc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(index, len(img_features.keys()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"beg_seq = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"[CLS] \"))\nend_seq = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\" [SEP]\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Caption_Generation(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.dense1 = nn.Sequential(nn.Linear(7*7*512,256), nn.Dropout(0.2), nn.ReLU())\n        \n        self.dense2 = nn.Sequential(nn.Linear(max_len*768,1024), nn.Dropout(0.2), nn.ReLU())\n        self.dense3 = nn.Sequential(nn.Linear(1024,256), nn.Dropout(0.2), nn.ReLU())\n        \n        self.cap_gen = nn.Sequential(\n            nn.Linear(256,128), nn.ReLU(),\n            nn.Linear(128,index), nn.ReLU()\n        )\n    \n    def forward(self, x1, x2):\n        desc = (beg_seq + x2 + end_seq)\n        att_mask = torch.IntTensor([[1]*len(desc) + [0]*(max_len-len(desc))]).to(device)\n        pad_desc = (desc + [0]*(max_len-len(desc)))\n        t_pad_desc = torch.IntTensor([pad_desc]).to(device)\n        bert_WE.eval()\n        \n        ip1 = self.flatten(x1)\n        ip1 = self.dense1(ip1)\n        \n        ip2 = (bert_WE(t_pad_desc, attention_mask = att_mask, output_all_encoded_layers = False))[0]\n        ip2 = self.flatten(ip2)\n        ip2 = self.dense2(ip2)\n        ip2 = self.dense3(ip2)\n        \n        ip = torch.add(ip1,ip2)\n        out = self.cap_gen(ip)\n        \n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(train_data, epochs, model, optim, loss_f):\n    \n    model = model.to(device)\n    model.train()\n    \n    for epoch in range(epochs):\n        train_loss = 0.0\n        idx = 0\n        for ip,op in train_data:\n            optim.zero_grad()\n            output = model(img_features[ip[0]],ip[1]).to(device)\n            \n            output = int(torch.max(output[-1].view(1,-1), 1)[1])\n            output = my_rev_vocab[output]\n            output = ip[1][1:]+tokenizer.convert_tokens_to_ids([output])\n            \n            t_output = torch.unsqueeze(torch.FloatTensor(output).to(device),0).requires_grad_()\n            t_op = torch.unsqueeze(torch.FloatTensor(op).to(device),0).requires_grad_()\n            # print(t_output, t_op)\n            \n            loss = loss_f(t_output,t_op)\n            train_loss += loss\n            \n            for params in model.parameters():\n                params.requires_grad = True\n            \n            loss.backward()\n            optim.step()\n                    \n        train_loss /= len(train_data)\n        print('Epoch: ', epoch, 'Avg Train loss: ', float(train_loss))\n        model.train()\n\ndef test(test_data, model, loss_f):\n    model.eval()\n    test_loss = 0.0\n    \n    for ip,op in test_data:\n        output = model(img_features[ip[0]],ip[1]).to(device)\n        output = int(torch.max(output[-1].view(1,-1), 1)[1])\n        output = my_rev_vocab[output]\n        print(output, end = ' ')\n        output = ip[1][1:]+tokenizer.convert_tokens_to_ids([output])\n        \n        t_output = torch.unsqueeze(torch.FloatTensor(output).to(device),0).requires_grad_()\n        t_op = torch.unsqueeze(torch.FloatTensor(op).to(device),0).requires_grad_()\n        # print(t_output, t_op)\n        \n        loss = loss_f(t_output,t_op)\n        test_loss += loss\n        print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_generation = Caption_Generation().to(device)\n\noptimizer = torch.optim.Adam(caption_generation.parameters(), lr = 0.001)\nloss = torch.nn.CrossEntropyLoss().to(device)\n\ntrain(data, 2, caption_generation, optimizer, loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test(data[:5], caption_generation, loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}