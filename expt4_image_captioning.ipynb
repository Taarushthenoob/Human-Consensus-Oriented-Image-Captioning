{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch-pretrained-bert\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms, datasets, models\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport re\nfrom scipy import ndimage\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport numpy as np\n\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel\nfrom tqdm import tqdm, trange\nimport pandas as pd\nimport io\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-19T12:01:25.523485Z","iopub.execute_input":"2022-05-19T12:01:25.523929Z","iopub.status.idle":"2022-05-19T12:01:46.485644Z","shell.execute_reply.started":"2022-05-19T12:01:25.523892Z","shell.execute_reply":"2022-05-19T12:01:46.484531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:01:46.487746Z","iopub.execute_input":"2022-05-19T12:01:46.488014Z","iopub.status.idle":"2022-05-19T12:01:46.493587Z","shell.execute_reply.started":"2022-05-19T12:01:46.487981Z","shell.execute_reply":"2022-05-19T12:01:46.492452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = transforms.Resize([224, 224])\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\nto_tensor = transforms.ToTensor()\n\nfeature_extraction = torchvision.models.resnet18(pretrained=True).to(device)\nfeature_extraction = nn.Sequential(*list(feature_extraction.children())[:-2]).to(device)\n\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_WE = BertModel.from_pretrained('bert-base-uncased').to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:01:46.494841Z","iopub.execute_input":"2022-05-19T12:01:46.495190Z","iopub.status.idle":"2022-05-19T12:01:56.195580Z","shell.execute_reply.started":"2022-05-19T12:01:46.495158Z","shell.execute_reply":"2022-05-19T12:01:56.194516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param in feature_extraction.parameters():\n    param.requires_grad_(False)\nfeature_extraction.eval()\n\ndef img_load_feat(img_loc,img_name):\n    img_loc += str(img_name) + '.jpg'\n    img = Image.open(img_loc)\n    t_img = normalize(to_tensor(scaler(img)))\n    t_img = t_img.to(device)\n    t_img = torch.unsqueeze(t_img, 0)\n    feature = feature_extraction(t_img)\n    return feature","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:02:44.603557Z","iopub.execute_input":"2022-05-19T12:02:44.604817Z","iopub.status.idle":"2022-05-19T12:02:44.612666Z","shell.execute_reply.started":"2022-05-19T12:02:44.604773Z","shell.execute_reply":"2022-05-19T12:02:44.611434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_loc = '../input/flickr8k/Images/'\nimg_features = dict()\nfor img_id_jpg in os.listdir(img_loc):\n    img_id = img_id_jpg.split('.')[0]\n    img_features[img_id] = img_load_feat(img_loc, img_id)\n    img_features[img_id].requires_grad_()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:02:45.416121Z","iopub.execute_input":"2022-05-19T12:02:45.417361Z","iopub.status.idle":"2022-05-19T12:11:54.146084Z","shell.execute_reply.started":"2022-05-19T12:02:45.417297Z","shell.execute_reply":"2022-05-19T12:11:54.144578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cider(captions):\n\n    cider = dict()\n    tfidfVectorizers=[]\n    for i in range(4):\n        tf_idf_vect = TfidfVectorizer(ngram_range = (i+1,i+1))\n        tfidfVectorizers.append(tf_idf_vect)\n    for key in captions:\n        corpus = captions[key]\n        cider[key] = []\n        \n        tfidf_matrix = tfidfVectorizers[0].fit_transform(corpus)        \n        tfidf_mat = tfidfVectorizers[0].fit_transform(corpus)\n        mean_cider=0\n        for i in range(len(corpus)):\n            mean_cider=0\n            for n in range(4):\n                tfidf_mat = tfidfVectorizers[0].fit_transform(corpus)\n                tfidf_mat = tfidf_mat.todense().tolist()\n                r = tfidf_mat[i] \n                r_norm = np.linalg.norm(np.array(r))\n                sum = 0\n                for j in range(len(corpus)):\n                    if i != j:\n                        vec_norm = np.linalg.norm(np.array(tfidf_mat[j]))\n                        sum += (np.dot(r,tfidf_mat[j])/(np.sqrt(r_norm*vec_norm)))\n                mean_cider += (sum/(len(corpus)-1))\n            mean_cider /= 4.0\n            cider[key].append(mean_cider)\n            \n    return cider","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:11:54.148807Z","iopub.execute_input":"2022-05-19T12:11:54.149207Z","iopub.status.idle":"2022-05-19T12:11:54.162473Z","shell.execute_reply.started":"2022-05-19T12:11:54.149154Z","shell.execute_reply":"2022-05-19T12:11:54.161232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file = open('../input/flickr8k/captions.txt', 'r')\nip_desc = file.read()\nfile.close()\n\nimg_cap = dict()\ndata = list()\nmax_len = 35\nindex = 1\nmy_vocab = dict()\nmy_rev_vocab = dict()\n\nmy_vocab[\"[PAD]\"] = 0\nmy_rev_vocab[0] = \"[PAD]\"\n\nfor line in ip_desc.split('\\n')[1:-1]:\n    if '\"' in line:\n        ip = re.split(r',(?=\")',line)\n    else:\n        ip = line.split(',')\n    \n    # image name\n    img_id = ip[0].split('.')[0]\n    \n    # cleaning desc\n    clean_desc = ''\n    for ch in ip[1]:\n        if ('A'<=ch and ch<='Z') or ('a'<=ch and ch<='z') or ch==' ':\n            clean_desc += ch\n    clean_desc = \"[CLS] \" + clean_desc.rstrip().lower() + \" [SEP]\"\n    \n    # truncating sentences with len > max_len\n    if len(clean_desc) > max_len:\n        clean_desc = clean_desc[:max_len]\n    \n    # mapping each img_id to a list of 5 captions\n    if img_id not in img_cap:\n        img_cap[img_id] = list()\n    img_cap[img_id].append(clean_desc)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:19:00.826580Z","iopub.execute_input":"2022-05-19T12:19:00.826940Z","iopub.status.idle":"2022-05-19T12:19:02.106919Z","shell.execute_reply.started":"2022-05-19T12:19:00.826904Z","shell.execute_reply":"2022-05-19T12:19:02.105760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_ciders = get_cider(img_cap)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:19:03.764283Z","iopub.execute_input":"2022-05-19T12:19:03.764689Z","iopub.status.idle":"2022-05-19T12:24:15.120650Z","shell.execute_reply.started":"2022-05-19T12:19:03.764652Z","shell.execute_reply":"2022-05-19T12:24:15.119276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for img_id in img_cap:\n    for cap_num in range(len(img_cap[img_id])):\n        clean_desc = img_cap[img_id][cap_num]\n        # tokenization of clean desc\n        tok_desc = clean_desc.split()\n        # tok_desc = tokenizer.tokenize(clean_desc)    \n\n        # building vocab\n        tok_desc_ind = list()\n        for tok in tok_desc:\n            if tok not in my_vocab:\n                my_vocab[tok] = index\n                my_rev_vocab[index] = tok\n                index += 1\n            tok_desc_ind.append(my_vocab[tok])\n\n        # data\n        for i in range(1,len(tok_desc_ind)-1):\n            data.append([[img_id,tok_desc_ind[:i]],tok_desc_ind[i],all_ciders[img_id][cap_num]])\n\n        tok_desc_ind += [0]*(max_len-len(tok_desc))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:24:15.123029Z","iopub.execute_input":"2022-05-19T12:24:15.123386Z","iopub.status.idle":"2022-05-19T12:24:16.599507Z","shell.execute_reply.started":"2022-05-19T12:24:15.123349Z","shell.execute_reply":"2022-05-19T12:24:16.598370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = data[:(80*len(data))//100]\ntest_data = data[(80*len(data))//100:]\n# train_data, test_data = torch.utils.data.random_split(img_ids, [int(0.8*len(img_ids)),len(img_ids)-int(0.8*len(img_ids))])\n\nprint(index, len(img_features.keys()))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:24:16.601277Z","iopub.execute_input":"2022-05-19T12:24:16.601650Z","iopub.status.idle":"2022-05-19T12:24:16.617015Z","shell.execute_reply.started":"2022-05-19T12:24:16.601604Z","shell.execute_reply":"2022-05-19T12:24:16.615821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConsensusLoss(torch.nn.Module):\n    \n    def _init_(self):\n        super(ConsensusLoss,self)._init_()\n        \n    def forward(self, outputs, targets, cider):\n        \n        den=0\n        op = torch.exp(outputs[0])\n        den = torch.sum(op)\n        op = torch.div(op, den)\n        \n        ce = -1*torch.log(op[targets[0]])\n        cl = (ce*cider)\n        \n        return cl","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:24:16.619970Z","iopub.execute_input":"2022-05-19T12:24:16.620303Z","iopub.status.idle":"2022-05-19T12:24:16.641535Z","shell.execute_reply.started":"2022-05-19T12:24:16.620268Z","shell.execute_reply":"2022-05-19T12:24:16.640618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Caption_Generation(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.img_proc = nn.Sequential(nn.Linear(7*7*512,4096), nn.Dropout(0.2), nn.ReLU(),\n                                    nn.Linear(4096,512), nn.Dropout(0.2), nn.ReLU(),\n                                    nn.Linear(512,128), nn.Dropout(0.2), nn.ReLU()\n                                   )\n        \n        self.lstm = nn.LSTM(768,(3072))\n        \n        self.cap_proc = nn.Sequential(\n                                    nn.Linear(3072,512), nn.Dropout(0.2), nn.ReLU(),\n                                    nn.Linear(512,128), nn.Dropout(0.2), nn.ReLU()\n                                   )\n        \n        self.cap_gen = nn.Sequential(\n            nn.Linear(256,128), nn.ReLU(),\n            nn.Linear(128,index), nn.ReLU()\n        )\n    \n    def forward(self, x1, x2):\n        \n        ip1 = self.flatten(x1)\n        ip1 = self.img_proc(torch.tensor(ip1))\n        \n        \n        desc = ([index-2] + x2 + [index-1])\n        att_mask = torch.IntTensor([[1]*len(desc) + [0]*(max_len-len(desc))]).to(device)\n        pad_desc = (desc + [0]*(max_len-len(desc)))\n        t_pad_desc = torch.IntTensor([pad_desc]).to(device)\n        bert_WE.eval()\n        \n        ip2 = (bert_WE(t_pad_desc, attention_mask = att_mask, output_all_encoded_layers = False))[0][0][-1]\n        ip2 = torch.tensor(torch.unsqueeze(ip2,0)).to(device)\n        ip2 = self.lstm(ip2)[0]\n        ip2 = self.cap_proc(ip2)\n        \n        ip = torch.unsqueeze(torch.cat((ip1[0],ip2[0]),0),0)\n        out = self.cap_gen(ip)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:24:16.643096Z","iopub.execute_input":"2022-05-19T12:24:16.643668Z","iopub.status.idle":"2022-05-19T12:24:16.660814Z","shell.execute_reply.started":"2022-05-19T12:24:16.643616Z","shell.execute_reply":"2022-05-19T12:24:16.659523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(train_data, epochs, model, optim, loss_f):\n    \n    model = model.to(device)\n    model.train()\n    loss_vals = list()\n    \n    for epoch in range(epochs):\n        train_loss = 0.0\n        idx = 0\n        for ip,op,score in train_data:\n            if ip[0] not in img_features:\n                continue\n            optim.zero_grad()\n            \n            output = model(img_features[ip[0]],ip[1]).to(device)            \n            t_output = torch.FloatTensor(output).to(device).requires_grad_()\n            t_op = torch.LongTensor([op]).to(device)\n            loss = loss_f(t_output,t_op,score)\n            \n            if idx%100 == 0:\n                print(\"idx: \", idx)\n                print(\"img_id: \", ip[0])\n                img = Image.open(img_loc+ip[0]+'.jpg')\n                imgplot = plt.imshow(img)\n                plt.show()\n                \n                print(\"ip:\", end = ' ')\n                for word in ip[1]:\n                    print(my_rev_vocab[word], end = ' ')\n                print()\n                word = int(torch.max(output[-1].view(1,-1), 1)[1])\n                word = my_rev_vocab[word]\n                print(\"actual op: \", word)\n                print(\"target op:\", my_rev_vocab[op])\n                print(\"loss: \", loss.item())\n                print(\"---------------------------------------\")\n            idx += 1\n                \n            train_loss += loss\n            loss.backward()\n            optim.step()\n        \n        train_loss /= len(train_data)\n        print('Epoch: ', epoch, 'Avg Train loss: ', float(train_loss))\n        loss_vals.append(train_loss)\n        \n    plt.plot(np.linspace(1, epochs, epochs).astype(int), loss_vals)\n        \n\ndef test(img_loc, test_data, model, loss_f):\n    model.eval()\n    test_loss = 0.0\n    \n    for ip,op,score in train_data:\n        if ip[0] not in img_features:\n            continue\n\n        output = model(img_features[ip[0]],ip[1]).to(device)\n\n        t_output = torch.FloatTensor(output).to(device).requires_grad_()\n        t_op = torch.LongTensor([op]).to(device)\n        loss = loss_f(t_output,t_op,score)\n        test_loss += loss\n        \n        # printing img, given input prefix sentence, predicted next word, expected next word\n        print(ip[0], loss.item())\n        img = Image.open(img_loc+ip[0]+'.jpg')\n        imgplot = plt.imshow(img)\n        plt.show()\n        print(\"ip:\", end = ' ')\n        for word in ip[1]:\n            print(my_rev_vocab[word], end = ' ')\n        print()\n        word = int(torch.max(output[-1].view(1,-1), 1)[1])\n        word = my_rev_vocab[word]\n        print(\"actual op: \", word)\n        print(\"target op:\", my_rev_vocab[op])\n        \n    test_loss /= len(test_data)\n    print('Epoch: ', epoch, 'Avg Test loss: ', float(test_loss))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:24:16.662854Z","iopub.execute_input":"2022-05-19T12:24:16.663199Z","iopub.status.idle":"2022-05-19T12:24:16.698259Z","shell.execute_reply.started":"2022-05-19T12:24:16.663148Z","shell.execute_reply":"2022-05-19T12:24:16.697121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_generation = Caption_Generation().to(device)\n\nprint(\"Total Parameters: \", sum(p.numel() for p in caption_generation.parameters()))\nprint(\"Trainable Parameters: \", sum(p.numel() for p in caption_generation.parameters() if p.requires_grad))\nprint()\n\noptimizer = torch.optim.Adam(caption_generation.parameters(), lr = 5e-4)\nloss = ConsensusLoss().to(device)\ntrain(train_data, 1, caption_generation, optimizer, loss)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:24:16.699751Z","iopub.execute_input":"2022-05-19T12:24:16.700124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test(img_loc, test_data, caption_generation, loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sample Test","metadata":{}},{"cell_type":"code","source":"def generate_caption(img_feature, caption, model):\n    model = model.to(device)\n    model.eval()\n    output = model(img_feature, caption)[0].to(device)\n    next_word = torch.max(output[-1].view(1,-1), 1)[1].to(device)\n    if next_word == vocab[\"[SEP]\"] or len(caption) == max_len-1:\n        caption.append(vocab[\"[SEP]\"])\n        return caption\n    caption.append(next_word)\n    return generate_caption(img_feature, caption, model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.seed(20)\nimg_id = data[random.randint(0,len(data)-1)][0][0]\n\nimg = Image.open(img_loc+ip[0]+'.jpg')\nimgplot = plt.imshow(img)\nplt.show()\n\ncaption = generate_caption(img_features[img_id], [my_vocab[\"[CLS]\"]], caption_generation)\nfor word_seq in caption[0]:\n    print(rev_vocab[int(word_seq)], end = ' ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References:\n* https://www.ijcai.org/proceedings/2020/0092.pdf\n* https://towardsdatascience.com/a-guide-to-image-captioning-e9fd5517f350\n* https://arxiv.org/pdf/1411.4555.pdf\n* https://www.analyticsvidhya.com/blog/2018/04/solving-an-image-captioning-task-using-deep-learning/\n* https://towardsdatascience.com/3-types-of-contextualized-word-embeddings-from-bert-using-transfer-learning-81fcefe3fe6d\n* https://arxiv.org/pdf/1502.03044.pdf","metadata":{}}]}