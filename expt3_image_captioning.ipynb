{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch-pretrained-bert\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms, datasets, models\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport re\nfrom scipy import ndimage\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport numpy as np\n\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel\nfrom tqdm import tqdm, trange\nimport pandas as pd\nimport io\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n# % matplotlib inline\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-18T12:35:20.401652Z","iopub.execute_input":"2022-05-18T12:35:20.402371Z","iopub.status.idle":"2022-05-18T12:35:44.425062Z","shell.execute_reply.started":"2022-05-18T12:35:20.402282Z","shell.execute_reply":"2022-05-18T12:35:44.423888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T12:35:44.427033Z","iopub.execute_input":"2022-05-18T12:35:44.427337Z","iopub.status.idle":"2022-05-18T12:35:44.433119Z","shell.execute_reply.started":"2022-05-18T12:35:44.427303Z","shell.execute_reply":"2022-05-18T12:35:44.431743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = transforms.Resize([224, 224])\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\nto_tensor = transforms.ToTensor()\n\nfeature_extraction = torchvision.models.resnet18(pretrained=True).to(device)\nfeature_extraction = nn.Sequential(*list(feature_extraction.children())[:-2]).to(device)\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_WE = BertModel.from_pretrained('bert-base-uncased').to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T12:35:44.435364Z","iopub.execute_input":"2022-05-18T12:35:44.436062Z","iopub.status.idle":"2022-05-18T12:36:35.002524Z","shell.execute_reply.started":"2022-05-18T12:35:44.436008Z","shell.execute_reply":"2022-05-18T12:36:35.001555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param in feature_extraction.parameters():\n    param.requires_grad = False\nfeature_extraction.eval()\n\ndef img_load_feat(img_loc,img_name):\n    img_loc += str(img_name) + '.jpg'\n    img = Image.open(img_loc)\n    t_img = normalize(to_tensor(scaler(img)))\n    t_img = t_img.to(device)\n    t_img = torch.unsqueeze(t_img, 0)\n    feature = feature_extraction(t_img)\n    return feature","metadata":{"execution":{"iopub.status.busy":"2022-05-18T12:36:35.004195Z","iopub.execute_input":"2022-05-18T12:36:35.004455Z","iopub.status.idle":"2022-05-18T12:36:35.013806Z","shell.execute_reply.started":"2022-05-18T12:36:35.004424Z","shell.execute_reply":"2022-05-18T12:36:35.011144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_loc = '../input/flickr8k/Images/'\nimg_features = dict()\nfor img_id_jpg in os.listdir(img_loc):\n    img_id = img_id_jpg.split('.')[0]\n    img_features[img_id] = img_load_feat(img_loc, img_id)\n    img_features[img_id].requires_grad_()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T12:36:35.015341Z","iopub.execute_input":"2022-05-18T12:36:35.015621Z","iopub.status.idle":"2022-05-18T12:43:21.386704Z","shell.execute_reply.started":"2022-05-18T12:36:35.015587Z","shell.execute_reply":"2022-05-18T12:43:21.385387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file = open('../input/flickr8k/captions.txt', 'r')\nip_desc = file.read()\nfile.close()\n\nimg_cap = dict()\nimg_cap_indexed = dict()\nmax_len = 15\nindex = 1\nmy_vocab = dict()\nmy_rev_vocab = dict()\n\nfor line in ip_desc.split('\\n')[1:-1]:\n    if '\"' in line:\n        ip = re.split(r',(?=\")',line)\n    else:\n        ip = line.split(',')\n    \n    # image name\n    img_id = ip[0].split('.')[0]\n    \n    # cleaning desc\n    clean_desc = ''\n    for ch in ip[1]:\n        if ('A'<=ch and ch<='Z') or ('a'<=ch and ch<='z') or ch==' ':\n            clean_desc += ch\n    clean_desc = clean_desc.rstrip().lower()\n    \n    # truncating sentences with len > max_len\n    if len(clean_desc) > max_len:\n        clean_desc = clean_desc[:max_len]\n    \n    # tokenization of clean desc\n    tok_desc = tokenizer.tokenize(clean_desc)    \n    \n    # mapping each img_id to a list of 5 captions\n    if img_id not in img_cap:\n        img_cap[img_id] = list()\n        img_cap_indexed[img_id] = list()\n    img_cap[img_id].append(clean_desc)\n    \n    # building vocab\n    tok_desc_ind = list()\n    for tok in tok_desc:\n        if tok not in my_vocab:\n            my_vocab[tok] = index\n            my_rev_vocab[index] = tok\n            index += 1\n        tok_desc_ind.append(my_vocab[tok])\n    tok_desc_ind += [0]*(max_len-len(tok_desc))\n    img_cap_indexed[img_id].append(tok_desc_ind)\n    \n    # converting tokens to IDs\n    tok_desc = tokenizer.convert_tokens_to_ids(tok_desc)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:35:33.122996Z","iopub.status.idle":"2022-05-18T14:35:33.123419Z","shell.execute_reply.started":"2022-05-18T14:35:33.123195Z","shell.execute_reply":"2022-05-18T14:35:33.123238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_ids = list(img_cap.keys())\ntrain_data = img_ids[:(80*len(img_ids))//100]\ntest_data = img_ids[(80*len(img_ids))//100:]\n# train_data, test_data = torch.utils.data.random_split(img_ids, [int(0.8*len(img_ids)),len(img_ids)-int(0.8*len(img_ids))])\n\nprint(index, len(img_features.keys()))","metadata":{"execution":{"iopub.status.busy":"2022-05-18T12:43:29.052526Z","iopub.execute_input":"2022-05-18T12:43:29.052774Z","iopub.status.idle":"2022-05-18T12:43:29.060700Z","shell.execute_reply.started":"2022-05-18T12:43:29.052742Z","shell.execute_reply":"2022-05-18T12:43:29.059653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cider(captions):\n\n    cider = dict()\n    tfidfVectorizers=[]\n    for i in range(4):\n        tf_idf_vect = TfidfVectorizer(ngram_range = (i+1,i+1))\n        tfidfVectorizers.append(tf_idf_vect)\n    for key in captions:\n        corpus = captions[key]\n        cider[key] = []\n        \n        tfidf_matrix = tfidfVectorizers[0].fit_transform(corpus)\n        # terms = tf_idf_vect.get_feature_names()\n        # print(tfidf_matrix)\n        \n        tfidf_mat = tfidfVectorizers[0].fit_transform(corpus)\n        mean_cider=0\n        for i in range(len(corpus)):\n            mean_cider=0\n            for n in range(4):\n                tfidf_mat = tfidfVectorizers[0].fit_transform(corpus)\n                # print(tfidf_mat.todense().tolist())\n                tfidf_mat = tfidf_mat.todense().tolist()\n                r = tfidf_mat[i] \n                r_norm = np.linalg.norm(np.array(r))\n                sum=0\n                for j in range(len(corpus)):\n                    if i != j:\n                        vec_norm = np.linalg.norm(np.array(tfidf_mat[j]))\n                        sum += (np.dot(r,tfidf_mat[j])/(np.sqrt(r_norm*vec_norm)))\n                mean_cider += (sum/(len(corpus)-1))\n            mean_cider /= 4.0   \n            cider[key].append(mean_cider)\n            \n    return cider","metadata":{"execution":{"iopub.status.busy":"2022-05-18T12:43:29.062412Z","iopub.execute_input":"2022-05-18T12:43:29.062774Z","iopub.status.idle":"2022-05-18T12:43:29.079537Z","shell.execute_reply.started":"2022-05-18T12:43:29.062725Z","shell.execute_reply":"2022-05-18T12:43:29.078539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConsensusLoss(torch.nn.Module):\n    \n    def _init_(self):\n        super(ConsensusLoss,self)._init_()\n    \n    def softmax(self, x):\n        exp_x = torch.exp(x)\n        sum_x = torch.sum(exp_x, dim=1, keepdim=True)\n        return exp_x/sum_x\n    \n    def log_softmax(self, x):\n        return torch.exp(x) - torch.sum(torch.exp(x), dim=1, keepdim=True)\n        \n    def forward(self, outputs, targets, ciders):\n    \n        num_words = targets.shape[1]\n        sentence_size = outputs.shape[0]\n        outputs = self.log_softmax(outputs)\n        # print(outputs)\n        # print(len(outputs))\n        cl = 0\n        \n        # print(len(ciders))\n        # print(targets)\n        # print(type(targets[0]))\n\n        for i in range(len(ciders)):\n            output = outputs[range(sentence_size), targets[i]]\n            # print(ciders[i])            \n            cl += (-((torch.sum(output)/num_words)*ciders[i]))\n            # print(cl)\n\n        return cl/len(ciders)\n\n\n        # # reshape labels to give a flat vector of length batch_size*seq_len\n        # labels = labels.view(-1)  \n\n        # # mask out 'PAD' tokens\n        # mask = (labels >= 0).float()\n\n        # # the number of tokens is the sum of elements in mask\n        # num_tokens = int(torch.sum(mask).data[0])\n\n        # # pick the values corresponding to labels and multiply by mask\n        # outputs = outputs[range(outputs.shape[0]), labels]*mask\n\n        # # cross entropy loss for all non 'PAD' tokens\n        # return -torch.sum(outputs)/num_tokens","metadata":{"execution":{"iopub.status.busy":"2022-05-18T15:17:44.471238Z","iopub.execute_input":"2022-05-18T15:17:44.471557Z","iopub.status.idle":"2022-05-18T15:17:44.487953Z","shell.execute_reply.started":"2022-05-18T15:17:44.471524Z","shell.execute_reply":"2022-05-18T15:17:44.486923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_ciders = get_cider(img_cap)\nprint(type(all_ciders))","metadata":{"execution":{"iopub.status.busy":"2022-05-18T12:43:29.100592Z","iopub.execute_input":"2022-05-18T12:43:29.101010Z","iopub.status.idle":"2022-05-18T12:48:30.951369Z","shell.execute_reply.started":"2022-05-18T12:43:29.100978Z","shell.execute_reply":"2022-05-18T12:48:30.950372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"beg_seq = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"[CLS] \"))\nend_seq = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\" [SEP]\"))\npad_seq = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\" [PAD] \"))\nprint(beg_seq, end_seq, pad_seq)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T12:49:10.883373Z","iopub.execute_input":"2022-05-18T12:49:10.883654Z","iopub.status.idle":"2022-05-18T12:49:10.890763Z","shell.execute_reply.started":"2022-05-18T12:49:10.883616Z","shell.execute_reply":"2022-05-18T12:49:10.889783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Caption_Generation(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.dense1 = nn.Sequential(nn.Linear(7*7*512,8192), nn.Dropout(0.2), nn.ReLU(),\n                                    nn.Linear(8192,1024), nn.Dropout(0.2), nn.ReLU(),\n                                    nn.Linear(1024,256), nn.Dropout(0.2), nn.ReLU()\n                                   )\n        \n        self.dense2 = nn.Sequential(nn.Linear((max_len+2)*768,4096), nn.Dropout(0.2), nn.ReLU(),\n                                    nn.Linear(4096,1024), nn.Dropout(0.2), nn.ReLU(),\n                                    nn.Linear(1024,256), nn.Dropout(0.2), nn.ReLU()\n                                   )\n        \n        self.cap_gen = nn.Sequential(\n            nn.Linear(512,256), nn.ReLU(),\n            nn.Linear(256,index), nn.ReLU()\n        )\n    \n    def forward(self, feat, idx):\n        output = list()\n        desc = list()\n        \n        for i in range(max_len):\n            ip1 = self.flatten(feat)\n            ip1 = self.dense1(ip1)\n            # print(desc)\n            \n            pad_desc = list()\n            pad_desc += beg_seq\n            pad_desc += tokenizer.convert_tokens_to_ids(desc) + end_seq\n            pad_desc += ([0]*(max_len-len(desc)))\n            \n            att_mask = torch.tensor([[int(i>0) for i in pad_desc]]).to(device)\n            t_pad_desc = torch.LongTensor([pad_desc]).to(device)\n            # print(\"att_mask: \", att_mask)\n            # print(\"pad_desc: \", pad_desc)\n            \n            bert_WE.eval()\n            ip2 = (bert_WE(t_pad_desc, attention_mask = att_mask, output_all_encoded_layers = False))[0]\n            ip2 = self.flatten(ip2)\n            ip2 = self.dense2(ip2)\n            \n            # ip = torch.add(ip1,ip2)\n            ip = torch.unsqueeze(torch.cat((ip1[0],ip2[0]),0),0)\n            out = self.cap_gen(ip)[0]\n            output.append(out)\n            \n            word = int(torch.max(out.view(1,-1), 1)[1])\n            word = my_rev_vocab[word]\n            desc.append(word)\n        \n        if idx%100 == 0:\n            print(idx, desc)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:52:37.858288Z","iopub.execute_input":"2022-05-18T14:52:37.858562Z","iopub.status.idle":"2022-05-18T14:52:37.875539Z","shell.execute_reply.started":"2022-05-18T14:52:37.858530Z","shell.execute_reply":"2022-05-18T14:52:37.874271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(train_data, all_ciders, epochs, model, optim, loss_f):\n    \n    model = model.to(device)\n    model.train()\n    loss_vals = list()\n    \n    for epoch in range(epochs):\n        train_loss = 0.0\n        idx = 0\n        for img_id in train_data:\n            if img_id not in img_features:\n                continue\n            optim.zero_grad()\n            \n            output = model(img_features[img_id], idx)\n            out = list()\n            for o in output:\n                out.append(o.tolist())\n            output = out\n            t_output = (torch.FloatTensor(output).to(device)).requires_grad_()\n            op = img_cap_indexed[img_id]\n            t_op = (torch.LongTensor(op).to(device))\n            # print(t_output, t_op)\n            \n            loss = loss_f(t_output, t_op, all_ciders[img_id])\n            if idx%100 == 0:\n                print(idx, img_id, loss.item())\n                img = Image.open(img_loc+img_id+'.jpg')\n                imgplot = plt.imshow(img)\n                plt.show()\n            idx += 1\n                \n            train_loss += loss\n            loss.backward()\n            optim.step()\n                    \n        train_loss /= len(train_data)\n        print('Epoch: ', epoch, 'Avg Train loss: ', float(train_loss))\n        loss_vals.append(train_loss)\n        \n    plt.plot(np.linspace(1, epochs, epochs).astype(int), loss_vals)\n        \n\ndef test(img_loc, test_data, model, loss_f):\n    model.eval()\n    test_loss = 0.0\n    \n    for img_id in test_data:\n        \n        img = Image.open(img_loc+img_id+'.jpg')\n        imgplot = plt.imshow(img)\n        plt.show()\n        \n        output = model(img_features[img_id]).to(device)        \n        t_output = (torch.FloatTensor(output).to(device),0)\n        op = img_cap_indexed[img_id]\n        t_op = (torch.LongTensor(op).to(device),0)\n        # print(t_output, t_op)\n        \n        for word_prob in output:\n            word = int(torch.max(word_prob[-1].view(1,-1), 1)[1])\n            word = my_rev_vocab[word]\n            print(word, end=' ')\n        print()\n\n        loss = loss_f(t_output,t_op)\n        test_loss += loss\n        \n    test_loss /= len(train_data)\n    print('Epoch: ', epoch, 'Avg Test loss: ', float(test_loss))","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:52:39.828494Z","iopub.execute_input":"2022-05-18T14:52:39.829138Z","iopub.status.idle":"2022-05-18T14:52:39.845998Z","shell.execute_reply.started":"2022-05-18T14:52:39.829075Z","shell.execute_reply":"2022-05-18T14:52:39.845050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_generation = Caption_Generation().to(device)\n\noptimizer = torch.optim.Adam(caption_generation.parameters(), lr = 0.001)\nloss = ConsensusLoss().to(device)\n\ntrain(train_data, all_ciders, 2, caption_generation, optimizer, loss)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T15:17:47.356844Z","iopub.execute_input":"2022-05-18T15:17:47.357127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test(img_loc, test_data, caption_generation, loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeding dimension 512\npositional embedding 2048\nattention layer 6","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}